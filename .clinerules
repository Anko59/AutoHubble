### Rules and Guidelines for Python Project Development

#### General Guidelines
1. **Python Version**: Use Python 3.11 and the typing features that come with it.
2. **Code Structure**: 
   - Keep files under **300 lines**. If a file grows too large, refactor it into smaller, more manageable modules.
   - Maintain clear separation of concerns: split code into modules for agents, navigation, scraper generation, data pipelines, and utility functions.
3. **Environment Management**:
   - Use **Poetry** for dependency and virtual environment management.
   - Add packages using the `poetry add <package-name>` command; **do not modify the `pyproject.toml` directly**.
   - Use **Docker** for containerization and ensure compatibility with the project’s dependencies.
4. **Code Formatting**:
   - Use **Ruff** for linting and code formatting. Configure it in `pyproject.toml` and run it regularly to ensure code quality.
5. **Project Documentation**:
   - Keep a detailed and up-to-date **README.md** with:
     - A project description.
     - Setup instructions (including Poetry and Docker).
     - A clear explanation of how to use the AI multi-agent system.
     - Examples of structured input and output for agents.
   - Include relevant links to Scrapy Zyte API documentation and usage examples.
6. **Testing**:
   - Automated tests are **not required**, but ensure thorough manual testing during development.
   - Validate scraper functionality through logs, data quality checks, and simulated requests.
   - The assistant should run the project at the end of tasks to make sure that it is working and correct any issue.
7. **Logging**:
   - Logging must be detailed, using **loguru**

---

#### AI Multi-Agent System Workflow
The project involves creating a multi-agent AI system for generating Python Scrapy scrapers based on user inputs. The workflow is divided into structured steps to ensure quality and iterative refinement.

##### Workflow Overview:

1. **Initial Setup**:
   - Copy the **base Scrapy project** into a new folder to start each scraping task.
   - Use the **Scrapy Zyte API** for web scraping, which simplifies request handling and avoids IP blocks.
   - The base Scrapy project includes pre-defined pipelines to save data as JSON.

2. **Website Navigation**:
   - Use **Selenium** (or an alternative) and a **large-context LLM** to navigate the website.
   - The navigator starts from the base page of the website to scrape
   - It analyses every request made by this page and it's content. It chooses which elements he wants to
   store in memory, because it might be useful to write the scraper or navigate the website.
   - It then chooses which links to follow or not follow to continue gathering information
   - Once it has enough information, it stops the navigation and writes a report
   - The report must contain the list of fields to extract, and detailed, complete and exact information
   explaining how to get this information. Such as the urls, protocols, parameters, html blocks, response structure
   - Objectives:
     - Identify fields to extract based on the user request.
     - Gather critical details like JSON API endpoints, response structures, and HTML blocks for scraping.

3. **Scraper Generation**:
   - The base scraper is copied in a project specific folder
   - The writing of the scraper is done using the report produced by the Navigator
   - The code generation agent is always informed in his context of the code of the scraper, with the content of each file
   - **Step 1**: Generate the navigation code to handle website traversal and locate data sources.
   - **Step 2**: Write the extraction logic to parse and store the required data fields.

4. **Testing and Debugging**:
   - Execute the scraper to gather logs and output data.
   - Use a general-purpose LLM to analyze logs and detect errors or missing data.
   - Iterate with the coding agent to fix issues until the scraper passes all tests.
   - The tester agent produces a report 

5. **Advanced Issue Resolution**:
   - If the coding agent struggles to resolve issues:
     - Call a smarter reasoning model for assistance.
     - Roll back to the Selenium navigation step to refine data gathering.

6. **Completion**:
   - Finalize the scraper project once all tests pass successfully.

---

#### Scrapy Zyte API Setup
To integrate Zyte's API into your Scrapy project, you need to follow these main steps:

### 1. Install the required package
First, install the `scrapy-zyte-api` package:
```bash
pip install scrapy-zyte-api
```

### 2. Add your Zyte API key
In your `settings.py`, define the API key for authentication:
```python
ZYTE_API_KEY = "your_api_key_here"
```

Alternatively, you can set the API key as an environment variable with the same name.

### 3. Enable the Zyte API middleware
To route your requests through Zyte's API, add the following to your `settings.py`:
```python
DOWNLOADER_MIDDLEWARES = {
    "scrapy_zyte_api.ZyteAPIDownloadMiddleware": 100,
}
```
This middleware automatically sends all requests through Zyte’s API. **No manual intervention is required for most requests**.

### 4. Use the `meta` argument for advanced configuration (optional)
By default, Zyte API works automatically with the settings defined above, but you can customize request handling with the `meta` argument in your Scrapy requests.

- **Automatic Request Parameters**: If you want Zyte to automatically map parameters based on your Scrapy request, use the `zyte_api_automap` key in `meta`:
  ```python
  meta={"zyte_api_automap": True}
  ```
  This is **enabled by default in transparent mode**, meaning you don't need to manually add this unless you want to override the default behavior.

- **Customizing Request Parameters**: You can also add specific parameters to be used with Zyte’s API:
  ```python
  meta={"zyte_api_automap": {"browserHtml": True}}
  ```
  This tells Zyte to capture the browser-rendered HTML of the page. By using a dictionary instead of `True`, you can fine-tune the parameters for each request.

### 5. Optional: Other configurations
While most setups don’t require changes, you can adjust other settings in `settings.py`, such as:
- **Retries**: Configure retry logic for failed requests.
- **Session Management**: If needed, you can set a session checker to manage sessions for your requests.

By default, Zyte API’s middleware and transparent mode will handle most use cases, so unless you have specific needs (like capturing HTML content or setting request headers), you do not need to modify each individual request.

For further details, check the [official documentation](https://scrapy-zyte-api.readthedocs.io/en/latest/).

---

#### Example of Agent Structured Output
The agents use structured outputs to take actions, leveraging the **OpenRouter OpenAI-style API**.
Every time an agent is called, it should have a pydantic object to fill. A precise response format.
Example:

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="OPENROUTER_API_KEY",
    base_url="https://openrouter.ai/api/v1"
)

class WebsiteFields(BaseModel):
    name: str
    url: str
    data_fields: list[str]

completion = client.beta.chat.completions.parse(
    model="google/gemini-flash-1.5-8b",
    messages=[
        {"role": "system", "content": "Identify the fields to extract from this website."},
        {"role": "user", "content": "I need to scrape product names, prices, and stock status from an e-commerce site."},
    ],
    response_format=WebsiteFields,
)

print(completion.choices[0].message.parsed)
```
You cannot write nested models for structured outputs. You can however use `Field`to provide a description.
You also cannot use `Optional` type for the fields. The API will respond with an empty set: `{}` if it wants
to signify an unfilled argument.

---

#### Base Scrapy Project Structure
The base Scrapy project should:
- Use the Scrapy Zyte API for robust request handling.
- Include pipelines to save items as JSON.
- Have predefined middlewares and settings optimized for flexibility.

Example folder structure:
```
base_scrapy_project/
├── scrapy.cfg
├── project_name/
│   ├── __init__.py
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── settings.py
│   ├── spiders/
│       ├── __init__.py
```

Example `pipelines.py`:
```python
import json

class JsonWriterPipeline:
    def open_spider(self, spider):
        self.file = open('output.json', 'w')

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
```

Add this pipeline in `settings.py`:
```python
ITEM_PIPELINES = {
    'project_name.pipelines.JsonWriterPipeline': 300,
}
```